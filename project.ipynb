{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNECTWeEzjMwhTRwkkfrla2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ca2eca9280e4ca0b94264ce2bd5a1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33fba35f2a3948dba6b9aef5e3882f5a",
              "IPY_MODEL_071cf78cd5fd4aa7b4fe1e76d5a3cec2",
              "IPY_MODEL_a24b9cb64d36478986e26dfe5dcfb929"
            ],
            "layout": "IPY_MODEL_14d5e243123644fea2e85cc4d189d32c"
          }
        },
        "33fba35f2a3948dba6b9aef5e3882f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9950ad2d53f34fb59d4a08b38527666b",
            "placeholder": "​",
            "style": "IPY_MODEL_fa27c892dbcf41a58d904d9d8755364e",
            "value": "100%"
          }
        },
        "071cf78cd5fd4aa7b4fe1e76d5a3cec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab22218ff90c4a29b29c26801bf33560",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef6d68435e544f269c3a540d0c7b9f6a",
            "value": 5
          }
        },
        "a24b9cb64d36478986e26dfe5dcfb929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e261c59171f14eeaac6dd3fdc5ba4f78",
            "placeholder": "​",
            "style": "IPY_MODEL_5845fa420c56483a9c4bbfc4c8c3cefe",
            "value": " 5/5 [03:21&lt;00:00, 39.78s/it]"
          }
        },
        "14d5e243123644fea2e85cc4d189d32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9950ad2d53f34fb59d4a08b38527666b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa27c892dbcf41a58d904d9d8755364e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab22218ff90c4a29b29c26801bf33560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef6d68435e544f269c3a540d0c7b9f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e261c59171f14eeaac6dd3fdc5ba4f78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5845fa420c56483a9c4bbfc4c8c3cefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanzoni1/new_project_deep/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create 'modules' directory\n",
        "os.makedirs(\"modules\", exist_ok=True)"
      ],
      "metadata": {
        "id": "b6ClhO1LeY4L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m22s8Ix5du91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm2-vnORB_EM",
        "outputId": "73d21ec0-0202-49ab-9443-80b88b7c5d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/data_setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modules/data_setup.py\n",
        "\"\"\"\n",
        "Utility functions for creating PyTorch DataLoaders for various types of datasets\n",
        "(e.g., image classification, generic datasets). This module provides flexibility to work\n",
        "with multiple dataset formats.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import Tuple, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: Optional[str] = None,\n",
        "    test_dir: Optional[str] = None,\n",
        "    dataset: Optional[Dataset] = None,\n",
        "    transform: Optional[transforms.Compose] = None,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = NUM_WORKERS,\n",
        "    shuffle_train: bool = True,\n",
        "    pin_memory: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Optional[List[str]]]:\n",
        "    \"\"\"\n",
        "    Creates PyTorch DataLoaders for training and testing data.\n",
        "\n",
        "    Args:\n",
        "        train_dir (Optional[str]): Path to training directory (for folder-based datasets like images).\n",
        "        test_dir (Optional[str]): Path to testing directory (for folder-based datasets like images).\n",
        "        dataset (Optional[Dataset]): A custom PyTorch Dataset (if using non-folder based datasets).\n",
        "        transform (Optional[transforms.Compose]): Transforms to be applied on the data.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        num_workers (int): Number of workers for DataLoader.\n",
        "        shuffle_train (bool): Whether to shuffle the training data.\n",
        "        pin_memory (bool): Whether to use pinned memory in DataLoader (recommended for CUDA).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[DataLoader, DataLoader, Optional[List[str]]]: Train DataLoader, Test DataLoader, and list of class names (if applicable).\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset is not None:\n",
        "        # Custom Dataset case\n",
        "        train_data, test_data = dataset['train'], dataset['test']\n",
        "        class_names = dataset.get('classes', None)\n",
        "    elif train_dir and test_dir:\n",
        "        # Image folder case (directory based)\n",
        "        train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "        test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "        class_names = train_data.classes\n",
        "    else:\n",
        "        raise ValueError(\"You must provide either `train_dir` and `test_dir` for folder-based datasets or a `dataset`.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle_train,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,  # No need to shuffle test data\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names\n",
        "\n",
        "\n",
        "def get_default_transforms(data_type: str = 'image') -> transforms.Compose:\n",
        "    \"\"\"\n",
        "    Provides default transforms for datasets based on the type of data.\n",
        "\n",
        "    Args:\n",
        "        data_type (str): Type of the dataset (e.g., 'image', 'text', etc.)\n",
        "\n",
        "    Returns:\n",
        "        transforms.Compose: Default transformations applied to the data.\n",
        "    \"\"\"\n",
        "    if data_type == 'image':\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Transforms for {data_type} are not implemented yet.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrFSTDnbeYsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/training_engine.py\n",
        "\"\"\"\n",
        "Generalized functions for training and evaluating a PyTorch model for classification and regression.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               criterion: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device,\n",
        "               task_type: str = \"classification\") -> Tuple[float, Optional[float]]:\n",
        "    \"\"\"Performs one training step, updating model weights and returning loss and accuracy (if classification).\"\"\"\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X)\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        if task_type == \"classification\":\n",
        "            y_pred_class = torch.argmax(y_pred, dim=1)\n",
        "            correct += (y_pred_class == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_loss /= len(dataloader)\n",
        "    train_acc = correct / total if task_type == \"classification\" else None\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              criterion: torch.nn.Module,\n",
        "              device: torch.device,\n",
        "              task_type: str = \"classification\") -> Tuple[float, Optional[float]]:\n",
        "    \"\"\"Performs one evaluation step, returning loss and accuracy (if classification).\"\"\"\n",
        "    model.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "\n",
        "            test_loss += criterion(y_pred, y).item()\n",
        "            if task_type == \"classification\":\n",
        "                y_pred_class = torch.argmax(y_pred, dim=1)\n",
        "                correct += (y_pred_class == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    test_acc = correct / total if task_type == \"classification\" else None\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          criterion: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          task_type: str = \"classification\") -> Dict[str, List[Optional[float]]]:\n",
        "    \"\"\"Runs full training and evaluation loop, returning metrics for each epoch.\"\"\"\n",
        "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model, train_dataloader, criterion, optimizer, device, task_type)\n",
        "        test_loss, test_acc = test_step(model, test_dataloader, criterion, device, task_type)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc if train_acc is not None else 'N/A'}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc if test_acc is not None else 'N/A'}\")\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8PuACOsdfEIC",
        "outputId": "1c172a5f-3b67-432f-d49a-5d12d9b1a487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/training_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6GIqfNdfEhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/utils.py\n",
        "\"\"\"\n",
        "Utility functions for PyTorch model training, saving, and additional tasks.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model's state_dict to the specified directory.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to save.\n",
        "    target_dir: Directory to save the model.\n",
        "    model_name: Name of the saved model file, should end with '.pth' or '.pt'.\n",
        "    \"\"\"\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model's state_dict\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(), f=model_save_path)\n",
        "\n",
        "\n",
        "def load_model(model: torch.nn.Module,\n",
        "               model_path: str,\n",
        "               device: torch.device = torch.device(\"cpu\")) -> torch.nn.Module:\n",
        "    \"\"\"Loads a PyTorch model's state_dict from a file.\n",
        "\n",
        "    Args:\n",
        "    model: The PyTorch model instance where the state_dict will be loaded.\n",
        "    model_path: Path to the model state_dict file.\n",
        "    device: The device to load the model on.\n",
        "\n",
        "    Returns:\n",
        "    The model with loaded parameters.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"[INFO] Loaded model from: {model_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_training_results(results: dict,\n",
        "                          target_dir: str,\n",
        "                          filename: str = \"training_results.json\"):\n",
        "    \"\"\"Saves the training metrics (loss/accuracy) into a JSON file.\n",
        "\n",
        "    Args:\n",
        "    results: A dictionary containing training/test loss and accuracy.\n",
        "    target_dir: Directory to save the results file.\n",
        "    filename: Name of the JSON file to save results in.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    save_path = target_dir_path / filename\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"[INFO] Saved training results to: {save_path}\")"
      ],
      "metadata": {
        "id": "hxEFaaxNfE4X",
        "outputId": "a61b1ba7-2d23-4682-ae01-610893c82876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EP823pYni1_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modules/predictions.py\n",
        "\"\"\"\n",
        "Utility functions to make predictions and plot results for PyTorch models.\n",
        "\n",
        "Reference: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional\n",
        "from PIL import Image\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def predict_and_plot(\n",
        "    model: torch.nn.Module,\n",
        "    class_names: List[str],\n",
        "    image_path: str,\n",
        "    image_size: Tuple[int, int] = (224, 224),\n",
        "    transform: Optional[torchvision.transforms.Compose] = None,\n",
        "    device: torch.device = device,\n",
        "):\n",
        "    \"\"\"Makes a prediction on an image and plots the result.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Pre-trained PyTorch model.\n",
        "        class_names (List[str]): List of class names for the classification task.\n",
        "        image_path (str): Path to the image file to predict on.\n",
        "        image_size (Tuple[int, int], optional): Resize dimensions for the image. Defaults to (224, 224).\n",
        "        transform (Optional[torchvision.transforms.Compose], optional): Optional transform to apply to the image. Defaults to None.\n",
        "        device (torch.device, optional): Device to run predictions on. Defaults to detected device (CUDA if available, else CPU).\n",
        "\n",
        "    Returns:\n",
        "        None. Displays the image with predicted class and probability.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open and display the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Use provided transform or create a default one (ImageNet normalization)\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Transform the image and add batch dimension\n",
        "    transformed_img = transform(img).unsqueeze(dim=0).to(device)\n",
        "\n",
        "    # Set model to evaluation mode and make a prediction\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        pred_logits = model(transformed_img)\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    pred_probs = torch.softmax(pred_logits, dim=1)\n",
        "\n",
        "    # Get the predicted class index\n",
        "    pred_label_idx = torch.argmax(pred_probs, dim=1).item()\n",
        "\n",
        "    # Plot the image and prediction result\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {class_names[pred_label_idx]} | Prob: {pred_probs[0][pred_label_idx]:.3f}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Mchg9t24i2nc",
        "outputId": "b440bdeb-950a-4083-fecf-7251bc43bd91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/predictions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "je8oMOKBj2b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Importing utility functions\n",
        "from modules.training_engine import train\n",
        "from modules.predictions import predict_and_plot\n",
        "from modules.utils import save_model\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1. Prepare Dataset and Dataloaders\n",
        "# Apply basic transformations for training and testing sets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Create DataLoader for training and test datasets\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Get class names for CIFAR-10\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "# 2. Initialize Model\n",
        "# We'll use a pre-trained ResNet18 model and adjust it for CIFAR-10 (10 output classes)\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=10)  # CIFAR-10 has 10 classes\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "C5wTIlFHj3vN",
        "outputId": "10a33b04-3fe4-4e4c-dddf-eb1e66f84911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 193MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PazJZsiTj3sr",
        "outputId": "be6232db-46f0-4700-fadb-e626f6f5a258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.module.Module.parameters</b><br/>def parameters(recurse: bool=True) -&gt; Iterator[Parameter]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</a>Return an iterator over module parameters.\n",
              "\n",
              "This is typically passed to an optimizer.\n",
              "\n",
              "Args:\n",
              "    recurse (bool): if True, then yields parameters of this module\n",
              "        and all submodules. Otherwise, yields only parameters that\n",
              "        are direct members of this module.\n",
              "\n",
              "Yields:\n",
              "    Parameter: module parameter\n",
              "\n",
              "Example::\n",
              "\n",
              "    &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined vars&quot;)\n",
              "    &gt;&gt;&gt; for param in model.parameters():\n",
              "    &gt;&gt;&gt;     print(type(param), param.size())\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L,)\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L, 1L, 5L, 5L)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2233);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Set Up Loss Function (Criterion) and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 4. Train the Model\n",
        "# Number of epochs to train the model for\n",
        "epochs = 5\n",
        "\n",
        "# Training loop\n",
        "results = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,  # Loss function\n",
        "    epochs=epochs,\n",
        "    device=device,\n",
        "    task_type=\"classification\"  # We're doing classification here\n",
        ")\n"
      ],
      "metadata": {
        "id": "06arwOtWj3qM",
        "outputId": "bfdc496c-2a93-4a44-9e9f-d4d45b33da41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "1ca2eca9280e4ca0b94264ce2bd5a1e5",
            "33fba35f2a3948dba6b9aef5e3882f5a",
            "071cf78cd5fd4aa7b4fe1e76d5a3cec2",
            "a24b9cb64d36478986e26dfe5dcfb929",
            "14d5e243123644fea2e85cc4d189d32c",
            "9950ad2d53f34fb59d4a08b38527666b",
            "fa27c892dbcf41a58d904d9d8755364e",
            "ab22218ff90c4a29b29c26801bf33560",
            "ef6d68435e544f269c3a540d0c7b9f6a",
            "e261c59171f14eeaac6dd3fdc5ba4f78",
            "5845fa420c56483a9c4bbfc4c8c3cefe"
          ]
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ca2eca9280e4ca0b94264ce2bd5a1e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: Train Loss: 1.1110, Train Acc: 0.62126, Test Loss: 0.8130, Test Acc: 0.7205\n",
            "Epoch 2/5: Train Loss: 0.8157, Train Acc: 0.7245, Test Loss: 0.7408, Test Acc: 0.7546\n",
            "Epoch 3/5: Train Loss: 0.7205, Train Acc: 0.7538, Test Loss: 0.6305, Test Acc: 0.7844\n",
            "Epoch 4/5: Train Loss: 0.6635, Train Acc: 0.77772, Test Loss: 0.6417, Test Acc: 0.7828\n",
            "Epoch 5/5: Train Loss: 0.6122, Train Acc: 0.79136, Test Loss: 0.6864, Test Acc: 0.7697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKJDkxUkj3nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Make Predictions on a Sample Image from the Dataset\n",
        "# Get a sample image from the test dataset\n",
        "def get_sample_image(dataloader: DataLoader, index: int = 0):\n",
        "    \"\"\"Fetch a sample image from the dataloader.\"\"\"\n",
        "    for images, labels in dataloader:\n",
        "        return images[index], labels[index]\n",
        "\n",
        "sample_image, _ = get_sample_image(test_loader, 1)\n",
        "\n",
        "# Save the sample image to a file\n",
        "from torchvision.utils import save_image\n",
        "save_image(sample_image, 'sample_image.png')\n",
        "\n",
        "# Run prediction and plot\n",
        "predict_and_plot(\n",
        "    model=model,\n",
        "    class_names=class_names,\n",
        "    image_path='sample_image.png',  # Path where we saved the sample image\n",
        "    image_size=(32, 32),\n",
        "    transform=transform_test,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "7UUvHlJIj3gn",
        "outputId": "60a78cae-1332-429c-ff6e-234f603d4d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAecklEQVR4nO3ce1iVZbrH8XslyFk0QJAwBFNTx8RIyyxRcyhPlellWU1qdek4NjNOdsIOnnPPmIYdPF270xhmaWbTLpvJssmONmqm06QOgdvRVPAMCXJ49h9u7lwukPcuCc3v57r8w5ffunnWYrF+vKyXx+eccwIAgIicU98LAACcPigFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAohdNUixYtZPjw4fW9jDNSjx495Be/+EWtufz8fPH5fPL888/rsYkTJ4rP5zul62nRooVMnDjxlM78Maru92OPPVbfS8FpiFKoxvPPPy8+n0//hYaGSuvWreWuu+6S3bt31/fy6sWiRYskOzu7vpfxs1ZVSFX/wsPDpV27dvLQQw/JoUOH6nt5nhw4cEBGjhwpcXFxEhERIT179pR169Z5vn1lZaXMnTtX0tLSJCwsTGJiYqRXr16yYcMGv9y0adPk2muvlfj4ePH5fDWW7omP6fHf06heUH0v4HQ2efJkSUlJkZKSEvnwww9l7ty58tZbb8mmTZskPDy8vpf3k1q0aJFs2rRJxo4dW99LOWWSk5PlyJEjEhwcXN9L8TN37lyJjIyUoqIi+dvf/ibTpk2T9957Tz766KNTfhZzKlVWVkq/fv1kw4YNcu+990psbKzMmTNHevToIWvXrpVWrVrVOuP222+XnJwcue222+Suu+6S4uJiWb9+vezZs8cv99BDD0lCQoJ06tRJ/vrXv9Y6t+oxrdKgQQP7HTxLUAon0adPH7nkkktEROTOO++UmJgYmTVrlrz++usydOjQam9TXFwsERERP+Uy8QOdrj8xDh48WGJjY0VE5Ne//rUMGjRIli1bJp9++ql07dq12tt899139f6DytKlS+Xjjz+WJUuWyODBg0VEZMiQIdK6dWuZMGGCLFq06KS3f+WVV+SFF16QZcuWycCBA0+azcvLkxYtWkhhYaHExcXVurbjH1OcHL8+MujVq5eIHHtCiogMHz5cIiMjJTc3V/r27StRUVFyyy23iMixn5qys7Olffv2EhoaKvHx8TJq1CjZv3+/30znnEydOlWSkpIkPDxcevbsKf/85z+r/fy5ubmSm5tb6zr37dsn99xzj3To0EEiIyOlUaNG0qdPn4BT8Kpfk+Xn5/sdf//998Xn88n7778vIsd+R//mm2/Ktm3b9PS7RYsWmt+zZ4/ccccdEh8fL6GhodKxY0d54YUX/GYe/3vsp59+WlJTUyU8PFwyMzNl+/bt4pyTKVOmSFJSkoSFhcl1110n+/btC7hvc+bMkfbt20tISIgkJibKmDFj5MCBA9U+DmvXrpXLL79cwsLCJCUlRebNm1ftmo5/T6EmL774oqSnp0tYWJice+65ctNNN8n27dtrvd2pcOLzruo9k7Vr10r37t0lPDxcxo8fLyLevhbHe/zxxyU5OVnCwsIkIyNDNm3a5PfxsrIy+frrr+Xbb7+tdZ1Lly6V+Ph4ueGGG/RYXFycDBkyRF5//XUpLS096e1nzZolXbp0kYEDB0plZaUUFxfXmD3++eeFc04OHTokbApdO0rBoOoFOSYmRo+Vl5fL1VdfLU2bNpXHHntMBg0aJCIio0aNknvvvVe6desms2fPlhEjRkhOTo5cffXVUlZWprd/5JFH5OGHH5aOHTvKjBkzJDU1VTIzM6v9hrjqqqvkqquuqnWd33zzjSxfvlz69+8vs2bNknvvvVc2btwoGRkZsnPnTvP9fvDBByUtLU1iY2Nl4cKFsnDhQn1/4ciRI9KjRw9ZuHCh3HLLLTJjxgyJjo6W4cOHy+zZswNm5eTkyJw5c+S3v/2tjBs3Tv7+97/LkCFD5KGHHpK3335b7r//fhk5cqS88cYbcs899/jdduLEiTJmzBhJTEyUmTNnyqBBg2T+/PmSmZnp95iKiOzfv1/69u0r6enp8qc//UmSkpJk9OjR8uyzz5rv/7Rp0+S2226TVq1ayaxZs2Ts2LHy7rvvSvfu3WsspFOpuufd3r17pU+fPpKWlibZ2dnSs2dP89fiz3/+szzxxBMyZswYycrKkk2bNkmvXr383jfbsWOHtG3bVrKysmpd5/r16+Xiiy+Wc87xf1np0qWLfPfdd7Jly5Yab3vo0CFZs2aNdO7cWcaPHy/R0dESGRkpqamp8sorr9T6uWuTmpoq0dHREhUVJbfeeutZ+96gJw4BnnvuOScibuXKla6goMBt377dLV682MXExLiwsDD3n//8xznn3LBhw5yIuAceeMDv9qtXr3Yi4nJycvyOv/32237H9+zZ4xo2bOj69evnKisrNTd+/HgnIm7YsGF+t09OTnbJycm1rr+kpMRVVFT4HcvLy3MhISFu8uTJAfczLy/PL7tq1SonIm7VqlV6rF+/ftV+7uzsbCci7sUXX9RjR48edV27dnWRkZHu0KFD+vlFxMXFxbkDBw5oNisry4mI69ixoysrK9PjQ4cOdQ0bNnQlJSXOue8fq8zMTL/79tRTTzkRcc8++6wey8jIcCLiZs6cqcdKS0tdWlqaa9q0qTt69Kjfmp577jnNTZgwwR3/bZGfn+8aNGjgpk2b5ne/N27c6IKCggKOVyc5OdlNmDCh1lzV5968ebMrKChweXl5bv78+S4kJMTFx8e74uJiv/s3b948v9tbvxbHP5edc+6zzz5zIuL+8Ic/6LGq7InPxepERES422+/PeD4m2++6UTEvf322zXedt26dU5EXExMjIuPj3dz5sxxOTk5rkuXLs7n87kVK1ZUe7uCggInIjU+vtnZ2e6uu+5yOTk5bunSpe73v/+9CwoKcq1atXIHDx6s9T6djSiFalS9WJ74Lzk52e+JXVUK27Zt87v97373OxcdHe327NnjCgoK/P5FRka6O++80znn3KJFi6r9ZtmzZ4/nb8TalJeXu8LCQldQUOAuuugid/311wfczx9TCpmZmS4hISGghF566SUnIu6NN95wzn3/4vKb3/zGL7d8+XInIm7GjBl+x6te4HJzc51z3z9Wb731ll+utLTUNWrUyA0aNEiPZWRkuKCgIFdUVOSXnTt3rhMR98knn/it6WSlMGvWLOfz+dzWrVsDvpZt27Z1vXv3DnhMTmQthRP/tW/f3v3jH//wu38hISGutLTU7/bWr8XQoUMD1nDppZe6Nm3a1LrW6pxzzjlu9OjRAcffffddJyLutddeq/G2H3zwgd7fTz/9VI8fPnzYxcbGum7dulV7u9pKoTo5OTlORNz06dM93+ZswhvNJ/H0009L69atJSgoSOLj46VNmzYBp8ZBQUGSlJTkd2zr1q1y8OBBadq0abVzq66k2LZtm4hIwFUZcXFx0qRJkx+87srKSpk9e7bMmTNH8vLypKKiQj92/K8gToVt27ZJq1atAh6Xtm3b6sePd/755/v9Pzo6WkREmjdvXu3xqvdgqua0adPGL9ewYUNJTU0N+DyJiYkBb/i3bt1aRI69l3DZZZd5uHfHvpbOuRqvnKmLK5deffVVadSokQQHB0tSUpK0bNkyIHPeeedJw4YN/Y5ZvxbV3afWrVv/4F/XhIWFVfu+QUlJiX78ZLcVEUlJSZFLL71Uj0dGRsqAAQPkxRdflPLycgkK+vEvWTfffLOMGzdOVq5cKQ888MCPnvdzQymcRJcuXfTqo5qEhIQEfBNWVlZK06ZNJScnp9rbeLla4sd49NFH5eGHH5bbb79dpkyZIueee66cc845MnbsWKmsrNRcTZc3Hl8ip1pNlwLWdNzV8xuDlZWV4vP5ZMWKFdWu8fjLHE+V7t2713qlzMleYOtLs2bNqn1DuupYYmJijbet+lh8fHzAx5o2bSplZWVSXFysPyz8WM2bN6/2QgZQCnWiZcuWsnLlSunWrdtJv3mTk5NF5NhPo6mpqXq8oKAg4Coli6VLl0rPnj3lmWee8Tt+4MABvxebqrORE98sPfEnSpGaCyQ5OVm+/PJLqays9CvHr7/+Wj9+KlTN2bx5s99jdfToUcnLy5PevXv75Xfu3BlweXDVG52WK1datmwpzjlJSUnRM43TlfVrsXXr1oAZW7ZsMV/ZUyUtLU1Wr14d8Pk/++wzCQ8PP+njl5iYKAkJCbJjx46Aj+3cuVNCQ0MlKirqB63rRM45yc/Pl06dOp2SeT83XH1UB4YMGSIVFRUyZcqUgI+Vl5fri3Dv3r0lODhYnnzySb+fiGv6y2Gvl6Q2aNAg4CfsJUuWBHzDVf1a4oMPPtBjFRUVsmDBgoCZERERcvDgwYDjffv2lV27dsnLL7+sx8rLy+XJJ5+UyMhIycjIqHW9XvTu3VsaNmwoTzzxhN99e+aZZ+TgwYPSr18/v3x5ebnMnz9f/3/06FGZP3++xMXFSXp6uufPe8MNN0iDBg1k0qRJAY+pc0727t37A+/RqWf9WixfvtzvObFmzRr57LPPpE+fPnrMcknq4MGDZffu3bJs2TI9VlhYKEuWLJEBAwZISEiIHq/uuXzjjTfK9u3b5Z133vG7/euvvy69evUKOCP3oqCgIODY3LlzpaCgQK655hrzvLMBZwp1ICMjQ0aNGiXTp0+XL774QjIzMyU4OFi2bt0qS5YskdmzZ8vgwYMlLi5O7rnnHpk+fbr0799f+vbtK+vXr5cVK1ZU++uDqstRT/y7ghP1799fJk+eLCNGjJDLL79cNm7cKDk5OX4/YYuItG/fXi677DLJysqSffv2ybnnniuLFy+W8vLygJnp6eny8ssvy9133y2dO3fW3/WOHDlS5s+fL8OHD5e1a9dKixYtZOnSpfLRRx9Jdnb2KfvpLi4uTrKysmTSpElyzTXXyLXXXiubN2+WOXPmSOfOneXWW2/1yycmJsof//hHyc/Pl9atW8vLL78sX3zxhSxYsMD0PkDLli1l6tSpkpWVJfn5+XL99ddLVFSU5OXlyWuvvSYjR44MuHS2vli/FhdccIFcccUVMnr0aCktLZXs7GyJiYmR++67TzNVl6QOGzas1r/nGDx4sFx22WUyYsQI+eqrr/QvmisqKmTSpEl+2eqey1lZWfLKK6/IoEGD5O6775bo6GiZN2+elJWVyaOPPup3+4ULF8q2bdvku+++E5FjP9hMnTpVRER+9atf6VlRcnKy3HjjjdKhQwcJDQ2VDz/8UBYvXixpaWkyatQo7w/u2aTe3uI+jVVdlfP555+fNDds2DAXERFR48cXLFjg0tPTXVhYmIuKinIdOnRw9913n9u5c6dmKioq3KRJk1yzZs1cWFiY69Gjh9u0aZNLTk7+UZekjhs3Tmd269bNffLJJy4jI8NlZGT4ZXNzc13v3r31ssfx48e7d955J+Dqo6KiInfzzTe7xo0b65VYVXbv3u1GjBjhYmNjXcOGDV2HDh38ruhx7vsrXk68yqjqSqclS5b4Ha/pa/DUU0+5Cy+80AUHB7v4+Hg3evRot3//fr9MRkaGXrHTtWtXFxoa6pKTk91TTz1V7ZpOdvVRlVdffdVdccUVLiIiwkVERLgLL7zQjRkzxm3evDkgeyLr1UcFBQUnzVXdv+pYvxYzZ850zZs3dyEhIe7KK690GzZsqDbr9Uq4ffv2uTvuuMPFxMS48PBwl5GRUe33UU3P5dzcXDdw4EDXqFEjFxYW5nr16uXWrFlT7WMg1VypdeLz9s4773Tt2rVzUVFRLjg42F1wwQXu/vvv18tzEcjnHH/iB9Slqh1vT6edUoGa8J4CAEBRCgAARSkAABTvKQAAFGcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAAAVVN8LAPDjHDHmw+pkFfi54EwBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgGKbC+AMt/DzMlN+ZOfgOloJfg44UwAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgGLvI+AncNiY32jIhja37WW035BtYpqMnwPOFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAon3PO1fcigNNBgTH/UqH37BHj7ELDXhRff27bRKN8/27P2UvaxptmT+kVZcrj9MOZAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFHsfAT+QZcehcuPsjYZsfpFt9rd53rNRobbZI1p5z4bZRuMnwpkCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAMU2FwAAxZkCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAABUkNeg7+r/sk0uKfeebRxjGt2+S7Ln7BMP9jXN7mVKAzjeYUP2c+Pswl3GG3h+dRMZEGsbHWaLn1E4UwAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgPK+O8i/ttkmbzfkQxubRv9z9WrP2bmNo02ze43pZsoDZ5pnDduSrfhLvmn21q25nrNF2w+aZu/autuUL96+xXN2+AM3mWY/d9ulpvyZhDMFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAMrnnHN1Mdgy1FcXCwDOEgXGfJ4hm2CcXWTIphhnr9puyz8++++es8seyzDNjrIt5YzCmQIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAAFSd7X2EQH/b+K3nbEKzZqbZF8Xa1vJxufdst+Ak23CxLCbeNrpZovdsq2TT6I6XdzHle17ZyXP28b62rydQXzhTAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKCCvAZ95z9pm1yw1xDebZvdrLXnaLpx64LJ87p5zvaNNI2WzA6nz1YHq9Zb0juM0615A+87hdiyIrLhg8tN+aE937d9AvxsbDFkWxln+4z5U40zBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKJ9zznkK+qw7cljynpbww/R91hS/9PJUz9lOobalzB2XYbtBHVpQ6D07Ku4243TL3kfRpsnp4x7xnF32WJpp9vmm9OmjwJhf8Bfvm0JtWv+Vafaq91Z7zu4usO2RFdPM9lxJaJ7sOVu0v9w0u7DwoOfshb0uNs1+Y8q1nrN1sZsaZwoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAVB1uc2Fh2y8i5MZZnrOxSRGm2TtmDjOkG5tmz8/O8Zy98Ja+ptkdYk1xKTFky2yjJcyQjTPOrksfH7DlV7+X7zn76QfrTLM//2SL5+yOL21bUUjJx4bwXttsiamjrIj1dUIu7uo5mt6zu2l0py6dPGeT2to2o3ikg/dskWmySJSHDGcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQQfW9ABGRlpmPmvL/Xjzac/aSsTNMs3dIS8/ZHm3bmWZf0KSB52yLoErT7Cb0ewDbbkMi03PeNeW/LSz3nG2SYnuudO7yS8/ZoSledrT5XuG3prhJrGGbn8hI22xrvnOK96z3XZKOCTZkjTtTyULDxmR5ebbZE9rWnuGVBACgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAyuecc56CPl+dLcLjEoDTyjeGbKpxtuU7ou6+M+32G7LrjbO97zR1zNcHvGffea/MNDtv62bP2cJ//a9pdtHhIs/ZCy86zzT7HxO61ZrhTAEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCACqqrwddPPVpXo+WIIVtinN3EmEegLw3ZSOPsuVu9Z/+z3Ta7JN+W/0Vb79mv82yzl079s/dwk1jT7PNuuMZzdtf2nabZFas/9h5e9z+m2SLet384xvLdX2obHWrYuCSlnWl0dIr3rSv+vS7XNFuEbS4AAAaUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAADlfe+ji980Df7lHcGes33mm0bLt7u8Z3cZNz+y7K5SbtmESWz7/Fg3pSoybgtTbsiGhtpmWxw8YLzBX971nj38jXG4zfIOV3oP79phG17wlSFs20Fqx8fbDGnr3jr/a8juNs6ONubjDVnjk7wkwns2b69p9EEp9pyNueh802wvOFMAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoLzvprCun2nwmGaWdLJptkiMIXuecbZlLZZ1iJSK4U/jrX92b30Mg7yvvbS58TFs1sJ71rLfhohIQk/v2aSLbLOjvG8vICIihw94z1rvp7QxZI1baMgaQ3ancXahIWt9jlsfxC2GrPUxbOw9WmJ8HoZ537oiIdY22gvOFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoLzvfVSnttVhfp1x9lnCso1MnnG2KW/dm6oONntRDYz5CkN2t3H2LmMegSwvb42Ns+MNyygxTY4J2ut9dJF1b6racaYAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQJ0m21zg7LWjjvP4+WhszA+oi0X8P8P2OeW2bXwaR3p/We7ZNdU02wvOFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoNj7CMAp1NJ7NOgm0+T+zSNN+fV5azxnd8hy02yREM/J8zr0NE3u1MX7fkYDrrPN9oIzBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKbS5QzwaY0rOzX/WcfXTaH02zdxesMuVFdhuy5cbZEYZshWlyuiR6zvaMameaXSTFnrPPHZ5pmv0/eSWmvE2oKR3d/GLP2bZdbI9hnxv6eM52SogzzfaCMwUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACifc855Cvp8db0WoFY74x70nG0WatvPRooO2PL7ve/zI3LQNLpMtnnOrpYdptkvGWb/t2nymSu6VUdTPrJVqufswBtvMs0ecMMvPWe7RjYxzY7ykOFMAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAKqi+FwBY/Klgmufs43W4Dqstxvxzhux/GWefsaKMe1mFNjbMDjGN7nTxRZ6zbbtebJodatjPqNw02RvOFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAotrnAGWVTfS/gOH8xZK+rs1WcuSKCbC8/xdZNHcpLPEcjG0eaRkc2jvCejfKeFRGxrMS48YcnnCkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAECx9xHOKCsN2UnG2RON+bNClDF/xHu0uNy4l1HjWFM8Iu4876ObxZtmB4V6f+ksOlxsm53gPcveRwCAOkUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFNtc4GdrYn0v4CfifTOHY0INW1fkGmebXlGa2F5+4lNs97RF23bel9LsfNvsVm08Z5NSkk2z40zpU48zBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKPY+Ak5DEYZsknF2/mHjDSyaGLKxjW2zgxoY895f3mIT4k2jI5t4/wrFBgXbZhuy5abJIl5WwpkCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAMU2F8APFGLIJhhn7zJkv7BsLSEipZZtLqyvEI29b9IQHRVtGh3bzLYVRViU960ogsJCTbPLy71vMLGr5IhpdkJomOdsM9NktrkAABhRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAACU551NnHN1uQ4AwGmAMwUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAID6P+y+P0SrfQCvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "eXdwX0Nfj3Ua",
        "outputId": "ea59a507-2d7f-461d-c353-ad0906aacd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohWgJF84pUmS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}